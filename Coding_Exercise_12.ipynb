{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Twelve: Texts, Three Ways\n",
    "\n",
    "For this week, you will sample the three methods we've explored (topic modeling, sentiment analysis, and Markov chain generation) using the same set of root texts. \n",
    "\n",
    "- Collect and import ten documents (novels work best, but anything goes!)\n",
    "- Using the topic modeling code as a starter, build a topic model of the documents (Pick a topic run word cloud)\n",
    "- Using the sentiment analysis code as a starter, run a sentiment analysis on sample fragments from the documents and compare (See what is interesting)\n",
    "- Using the Markov chain code as a starter, generate a sentence using one of the documents\n",
    "- Using the Markov chain code as a starter, generate a longer text fragment using all of the documents\n",
    "\n",
    "As a bonus, try to extend this analysis to note other features of these documents using any of our previous exercises as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing 10 works by Arthur Conan Doyle, as used in last week's exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Stoddard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\'\n",
    "os.chdir(textdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\adventure.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\boer.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\fear.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\hound.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\last.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\memoirs.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\return.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\scarlet.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\sign.txt', 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\white.txt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "documents = []\n",
    "path = 'C:\\\\Users\\\\Stoddard\\\\DesignDevExercises\\\\text\\\\'\n",
    "\n",
    "filenames=sorted([os.path.join(path, fn) for fn in os.listdir(path)])\n",
    "print(len(filenames))\n",
    "print(filenames[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer=text.CountVectorizer(input='filenames', stop_words=\"english\", min_df=1)\n",
    "dtm=vectorizer.fit_transform(filenames).toarray() # defines document term matrix\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix: (10, 14). Number of tokens 59\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of document-term matrix: {dtm.shape}. '# in this case the shape is 7 b/c 7 docs\n",
    "      f'Number of tokens {dtm.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomposition\n",
    "model = decomposition.LatentDirichletAllocation(\n",
    "    n_components=100, learning_method='online', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_distributions = model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing a set of vocab\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# (# topics, # vocabulary)\n",
    "assert model.components_.shape == (100, len(vocabulary))\n",
    "# (# documents, # topics)\n",
    "assert document_topic_distributions.shape == (dtm.shape[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          adventure      boer  designdevexercises      fear     hound  \\\n",
      "Topic 0    0.237354  0.192002            0.193579  0.183461  0.239989   \n",
      "Topic 1    0.221949  0.190651            0.198547  0.214334  0.190587   \n",
      "Topic 2    0.209695  0.196927            0.296181  0.215479  0.254467   \n",
      "Topic 3    0.210005  0.166724            0.208297  0.219074  0.199425   \n",
      "Topic 4    0.195116  0.228752            0.183062  0.207091  0.218632   \n",
      "...             ...       ...                 ...       ...       ...   \n",
      "Topic 95   0.203013  0.204640            0.219346  0.195669  0.177860   \n",
      "Topic 96   0.198401  0.226942            0.209587  0.200464  0.222647   \n",
      "Topic 97   0.197749  0.180549            0.218141  0.200016  0.183130   \n",
      "Topic 98   0.205529  0.209199            0.205392  0.215759  0.170955   \n",
      "Topic 99   0.164080  0.197547            0.184811  0.224982  0.247049   \n",
      "\n",
      "           memoirs    return   scarlet      sign  stoddard      text  \\\n",
      "Topic 0   0.189205  0.233837  0.166079  0.226830  0.182971  0.200391   \n",
      "Topic 1   0.187645  0.182655  0.208383  0.200015  0.186861  0.204749   \n",
      "Topic 2   0.221622  0.213980  0.249399  0.226785  0.331580  0.346297   \n",
      "Topic 3   0.199842  0.207665  0.206094  0.206150  0.226740  0.196486   \n",
      "Topic 4   0.185644  0.178048  0.209995  0.210705  0.179136  0.202994   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "Topic 95  0.187243  0.201142  0.219543  0.208690  0.181925  0.210018   \n",
      "Topic 96  0.183279  0.240067  0.213238  0.203118  0.212319  0.182203   \n",
      "Topic 97  0.189668  0.190279  0.188611  0.219266  0.223675  0.157368   \n",
      "Topic 98  0.222162  0.196667  0.219125  0.190298  0.172075  0.169496   \n",
      "Topic 99  0.229819  0.196578  0.183898  0.187090  0.210109  0.248958   \n",
      "\n",
      "               txt     users     white  \n",
      "Topic 0   0.187041  0.182958  0.227062  \n",
      "Topic 1   0.191528  0.206117  0.216103  \n",
      "Topic 2   0.274127  0.303402  0.221436  \n",
      "Topic 3   0.191495  0.197088  0.204608  \n",
      "Topic 4   0.173754  0.203269  0.188945  \n",
      "...            ...       ...       ...  \n",
      "Topic 95  0.191876  0.231220  0.173391  \n",
      "Topic 96  0.217499  0.192474  0.221122  \n",
      "Topic 97  0.214193  0.219031  0.176381  \n",
      "Topic 98  0.179495  0.219037  0.188538  \n",
      "Topic 99  0.211059  0.215812  0.178476  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "topic_names = [f'Topic {k}' for k in range(100)]\n",
    "topic_word_distributions = pd.DataFrame(\n",
    "    model.components_, columns=vocabulary, index=topic_names)\n",
    "print(topic_word_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                  0.508072\n",
       "designdevexercises    0.495588\n",
       "txt                   0.490907\n",
       "users                 0.480355\n",
       "stoddard              0.466878\n",
       "boer                  0.308664\n",
       "adventure             0.257620\n",
       "white                 0.257170\n",
       "scarlet               0.250210\n",
       "memoirs               0.209963\n",
       "sign                  0.209306\n",
       "return                0.201594\n",
       "fear                  0.198173\n",
       "hound                 0.179849\n",
       "Name: Topic 7, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 7'].sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic 0   Topic 1   Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  \\\n",
      "0  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "1  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "2  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "3  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "4  0.001667  0.001667  0.001667  0.001667  0.001667  0.001667  0.001667   \n",
      "5  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "6  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "7  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "8  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "9  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429   \n",
      "\n",
      "    Topic 7   Topic 8   Topic 9  ...  Topic 90  Topic 91  Topic 92  Topic 93  \\\n",
      "0  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "1  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "2  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "3  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "4  0.001667  0.001667  0.001667  ...  0.001667  0.001667  0.001667  0.001667   \n",
      "5  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "6  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "7  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "8  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "9  0.001429  0.001429  0.001429  ...  0.001429  0.001429  0.001429  0.001429   \n",
      "\n",
      "   Topic 94  Topic 95  Topic 96  Topic 97  Topic 98  Topic 99  \n",
      "0  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "1  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "2  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "3  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "4  0.001667  0.001667  0.001667  0.001667  0.001667  0.001667  \n",
      "5  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "6  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "7  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "8  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "9  0.001429  0.001429  0.001429  0.001429  0.001429  0.001429  \n",
      "\n",
      "[10 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "document_topic_distributions = pd.DataFrame(\n",
    "    document_topic_distributions, columns=topic_names)\n",
    "print(document_topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                  0.508072\n",
       "designdevexercises    0.495588\n",
       "txt                   0.490907\n",
       "users                 0.480355\n",
       "stoddard              0.466878\n",
       "boer                  0.308664\n",
       "adventure             0.257620\n",
       "white                 0.257170\n",
       "scarlet               0.250210\n",
       "memoirs               0.209963\n",
       "sign                  0.209306\n",
       "return                0.201594\n",
       "fear                  0.198173\n",
       "hound                 0.179849\n",
       "Name: Topic 7, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = topic_word_distributions.loc['Topic 7'].sort_values(ascending=False).head(18)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-802f44b98a08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# requires install of word cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from wordcloud import wordcloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "print(topic_word_distributions.loc['Topic 7'].head(20))\n",
    "\n",
    "\n",
    "wordcloud = WordCloud().generate_from_frequencies(words)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da26090e65dfd4cdfd0711879594308b8318542197956b2adc85226ab711834"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
