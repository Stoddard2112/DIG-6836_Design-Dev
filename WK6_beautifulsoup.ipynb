{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Beautiful Soup is a more complicated program to more easily scrape webpages\r\n",
    "# Reads the scrape through pandas as a json\r\n",
    "#documented at: https://towardsdatascience.com/web-scraping-metacritic-reviews-using-beautifulsoup-63801bbe200e\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Same method of scarping a page as earlier.\r\n",
    "url = 'https://www.metacritic.com/game/switch/animal-crossing-new-horizons/user-reviews?page=0'\r\n",
    "\r\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\r\n",
    "response = requests.get(url, headers = user_agent)\r\n",
    "\r\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creates the headings on a spreadsheet\r\n",
    "review_dict = {'name':[], 'date':[], 'rating':[], 'review':[]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#look for review_content tags since every user review has this tag\r\n",
    " \r\n",
    "# Big to small sections.  This looks in the boxes named review_contnet.  If data is outside the windows of that box, it will not pull it, as long as it has a name, date, rating review (Catagories labeld in previous step) in the class.  Envision filling in the columns of a spreadsheet based on step two's headings\r\n",
    "for review in soup.find_all('div', class_='review_content'): \r\n",
    "    if review.find('div', class_='name') == None:\r\n",
    "        break \r\n",
    "    review_dict['name'].append(review.find('div', class_='name').find('a').text)\r\n",
    "    review_dict['date'].append(review.find('div', class_='date').text)\r\n",
    "    review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\r\n",
    "    if review.find('span', class_='blurb blurb_expanded'): \r\n",
    "        review_dict['review'].append(review.find('span', class_='blurb blurb_expanded').text)\r\n",
    "        print(review.find('span', class_='blurb blurb_expanded').text)\r\n",
    "    # FOr blurbs that link/open full text of review\r\n",
    "    elif review.find('div',class_='review_body').find('span') == None:\r\n",
    "        review_dict['review'].append('No review text.')\r\n",
    "        print(\"No review\")\r\n",
    "    else:\r\n",
    "        review_dict['review'].append(review.find('div',class_='review_body').find('span').text)\r\n",
    "        # Prints the reviews to see that what we did worked.\r\n",
    "        print(review.find('div',class_='review_body').find('span').text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Takes from the Beautiful Soup scrape into a pd spreadsheet, nice and structured as a dataframe\r\n",
    "ac_reviews = pd.DataFrame(review_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prints the new dataframe \r\n",
    "ac_reviews"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0da26090e65dfd4cdfd0711879594308b8318542197956b2adc85226ab711834"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}