{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Thirteen: Interface\n",
    "\n",
    "This week, you'll be revisiting a database (such as our example, gathered from Tweepy) and exploring methods for visualizing the data to others. Your exercise should:\n",
    "\n",
    "- Import or collect your data as appropriate, using OS or an API\n",
    "- Make and structure your data in a Pandas dataframe\n",
    "- Use NLTK to tokenize the data, and chart a word cloud\n",
    "- Create a \"wordcloud of interest\" by playing with the visualization methods from the class demo, or others documented in the API\n",
    "- Import Bokeh and chart some aspect of the text: this could be the wordcount, topics, or sentiment analysis as demoed\n",
    "\n",
    "Consider exploring other visualization types in the Bokeh API documentation, and play with the color options and scale of your visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of TAGS scrapped data on Spider-Man related hashtag No Way Home.  #NoWayHome was a top 10 trender on the morning of 11/16/2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.read_csv(\"nowayhome.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make and structure #NoWayHome data in a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>from_user</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>time</th>\n",
       "      <th>geo_coordinates</th>\n",
       "      <th>user_lang</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>from_user_id_str</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>source</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_location</th>\n",
       "      <th>status_url</th>\n",
       "      <th>entities_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.460627e+18</td>\n",
       "      <td>Isaa_Silvaa_002</td>\n",
       "      <td>RT @CamiliV72679845: Todo dia tenho pesadelos ...</td>\n",
       "      <td>Tue Nov 16 15:14:39 +0000 2021</td>\n",
       "      <td>16/11/2021 15:14:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.357343e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1432291119...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Minas Gerais, Brasil</td>\n",
       "      <td>http://twitter.com/Isaa_Silvaa_002/statuses/14...</td>\n",
       "      <td>{\"hashtags\":[{\"text\":\"NoWayHome\",\"indices\":[68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.460627e+18</td>\n",
       "      <td>lez_lepiej</td>\n",
       "      <td>Chyba wiem kto stoi za przeciekami i tym caÅ‚ym...</td>\n",
       "      <td>Tue Nov 16 15:14:38 +0000 2021</td>\n",
       "      <td>16/11/2021 15:14:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.298886e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1446088656...</td>\n",
       "      <td>369.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://twitter.com/lez_lepiej/statuses/1460627...</td>\n",
       "      <td>{\"hashtags\":[{\"text\":\"NoWayHome\",\"indices\":[57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.460627e+18</td>\n",
       "      <td>thejere07</td>\n",
       "      <td>RT @juandavidrrz: yo con la esperanza de ver a...</td>\n",
       "      <td>Tue Nov 16 15:14:35 +0000 2021</td>\n",
       "      <td>16/11/2021 15:14:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.194261e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8979544770...</td>\n",
       "      <td>154.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://twitter.com/thejere07/statuses/14606273...</td>\n",
       "      <td>{\"hashtags\":[{\"text\":\"Spiderman\",\"indices\":[11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.460627e+18</td>\n",
       "      <td>KlNG_Rayquan</td>\n",
       "      <td>RT @spidershollnd: I deserve a date from Tobey...</td>\n",
       "      <td>Tue Nov 16 15:14:35 +0000 2021</td>\n",
       "      <td>16/11/2021 15:14:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.596342e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1412648229...</td>\n",
       "      <td>769.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "      <td>http://twitter.com/KlNG_Rayquan/statuses/14606...</td>\n",
       "      <td>{\"hashtags\":[],\"symbols\":[],\"user_mentions\":[{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.460627e+18</td>\n",
       "      <td>Alexand12477863</td>\n",
       "      <td>RT @LynksMultiverse: His return was foretold ðŸ‘€...</td>\n",
       "      <td>Tue Nov 16 15:14:35 +0000 2021</td>\n",
       "      <td>16/11/2021 15:14:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.515083e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1455442092...</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://twitter.com/Alexand12477863/statuses/14...</td>\n",
       "      <td>{\"hashtags\":[{\"text\":\"mcu\",\"indices\":[48,52]},...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id_str        from_user  \\\n",
       "0  1.460627e+18  Isaa_Silvaa_002   \n",
       "1  1.460627e+18       lez_lepiej   \n",
       "2  1.460627e+18        thejere07   \n",
       "3  1.460627e+18     KlNG_Rayquan   \n",
       "4  1.460627e+18  Alexand12477863   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @CamiliV72679845: Todo dia tenho pesadelos ...   \n",
       "1  Chyba wiem kto stoi za przeciekami i tym caÅ‚ym...   \n",
       "2  RT @juandavidrrz: yo con la esperanza de ver a...   \n",
       "3  RT @spidershollnd: I deserve a date from Tobey...   \n",
       "4  RT @LynksMultiverse: His return was foretold ðŸ‘€...   \n",
       "\n",
       "                       created_at                 time  geo_coordinates  \\\n",
       "0  Tue Nov 16 15:14:39 +0000 2021  16/11/2021 15:14:39              NaN   \n",
       "1  Tue Nov 16 15:14:38 +0000 2021  16/11/2021 15:14:38              NaN   \n",
       "2  Tue Nov 16 15:14:35 +0000 2021  16/11/2021 15:14:35              NaN   \n",
       "3  Tue Nov 16 15:14:35 +0000 2021  16/11/2021 15:14:35              NaN   \n",
       "4  Tue Nov 16 15:14:35 +0000 2021  16/11/2021 15:14:35              NaN   \n",
       "\n",
       "   user_lang  in_reply_to_user_id_str in_reply_to_screen_name  \\\n",
       "0        NaN                      NaN                     NaN   \n",
       "1        NaN                      NaN                     NaN   \n",
       "2        NaN                      NaN                     NaN   \n",
       "3        NaN                      NaN                     NaN   \n",
       "4        NaN                      NaN                     NaN   \n",
       "\n",
       "   from_user_id_str  in_reply_to_status_id_str  \\\n",
       "0      1.357343e+18                        NaN   \n",
       "1      1.298886e+18                        NaN   \n",
       "2      1.194261e+08                        NaN   \n",
       "3      7.596342e+17                        NaN   \n",
       "4      8.515083e+17                        NaN   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                   profile_image_url  user_followers_count  \\\n",
       "0  http://pbs.twimg.com/profile_images/1432291119...                  20.0   \n",
       "1  http://pbs.twimg.com/profile_images/1446088656...                 369.0   \n",
       "2  http://pbs.twimg.com/profile_images/8979544770...                 154.0   \n",
       "3  http://pbs.twimg.com/profile_images/1412648229...                 769.0   \n",
       "4  http://pbs.twimg.com/profile_images/1455442092...                 248.0   \n",
       "\n",
       "   user_friends_count         user_location  \\\n",
       "0                69.0  Minas Gerais, Brasil   \n",
       "1               116.0                   NaN   \n",
       "2               121.0                   NaN   \n",
       "3               893.0         Charlotte, NC   \n",
       "4              1288.0                   NaN   \n",
       "\n",
       "                                          status_url  \\\n",
       "0  http://twitter.com/Isaa_Silvaa_002/statuses/14...   \n",
       "1  http://twitter.com/lez_lepiej/statuses/1460627...   \n",
       "2  http://twitter.com/thejere07/statuses/14606273...   \n",
       "3  http://twitter.com/KlNG_Rayquan/statuses/14606...   \n",
       "4  http://twitter.com/Alexand12477863/statuses/14...   \n",
       "\n",
       "                                        entities_str  \n",
       "0  {\"hashtags\":[{\"text\":\"NoWayHome\",\"indices\":[68...  \n",
       "1  {\"hashtags\":[{\"text\":\"NoWayHome\",\"indices\":[57...  \n",
       "2  {\"hashtags\":[{\"text\":\"Spiderman\",\"indices\":[11...  \n",
       "3  {\"hashtags\":[],\"symbols\":[],\"user_mentions\":[{...  \n",
       "4  {\"hashtags\":[{\"text\":\"mcu\",\"indices\":[48,52]},...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re_list = ['(https?://)?(www\\.)?(\\w+\\.)?(\\w+)(\\.\\w+)(/.+)?', '@[A-Za-z0-9_]+','#']\n",
    "combined_re = re.compile( '|'.join( re_list) )\n",
    "regex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLTK to tokenize the data, and chart a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token = WordPunctTokenizer()\n",
    "def cleaning_tweets(t):\n",
    "    del_amp = BeautifulSoup(t, 'lxml')\n",
    "    del_amp_text = del_amp.get_text()\n",
    "    del_link_mentions = re.sub(combined_re, '', del_amp_text)\n",
    "    del_emoticons = re.sub(regex_pattern, '', del_link_mentions)\n",
    "    lower_case = del_emoticons.lower()\n",
    "    words = token.tokenize(lower_case)\n",
    "    result_words = [x for x in words if len(x) > 2]\n",
    "    return (\" \".join(result_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Tweet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2e07623b3909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcleaned_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# After getting this error, I tried replacing Tweet with nowayhome, tweet, and empty brackets.  All produced the same type of error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcleaned_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaning_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Tweet'"
     ]
    }
   ],
   "source": [
    "cleaned_tweets = []\n",
    "for i in range(0,len(df_tweets['Tweet'])):\n",
    "    # After getting this error, I tried replacing Tweet with nowayhome, tweet, and empty brackets.  All produced the same type of error.\n",
    "    cleaned_tweets.append(cleaning_tweets((df_tweets.Tweet[i])))\n",
    "print(cleaned_tweets[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"wordcloud of interest\" by playing with the visualization methods from the class demo, or others documented in the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Bokeh and chart some aspect of the text: this could be the wordcount, topics, or sentiment analysis as demoed\n",
    "\n",
    "Consider exploring other visualization types in the Bokeh API documentation, and play with the color options and scale of your visualization."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da26090e65dfd4cdfd0711879594308b8318542197956b2adc85226ab711834"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
