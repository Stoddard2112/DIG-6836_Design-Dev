{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reviews preprocessing pipeline\r\n",
    "\r\n",
    "This notebook takes a collection of reviews, collects the text contnents, and cleans/sorts the resulting words ina dictionary.\r\n",
    "\r\n",
    "Request url with any url.  Replace starts ans stops w/ corresponding first/last words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Pulls a series of urls for analysis.  Declutters with start/stop sections.\r\n",
    "my_urls = [\"https://www.eurogamer.net/articles/2021-09-10-in-defence-of-god-of-wars-thor\",\"https://playcrazygame.com/2021/09/13/ragnarok-explain-the-god-thors-look/\",\"https://codelist.biz/2021/09/13/thor-is-just-as-thick-as-in-avengers-endgame/\"]\r\n",
    "\r\n",
    "starts = [\"Last \", \"The revelation\",\"The secret\"]\r\n",
    "stops = [\"bad joke.\", \"without a defined date.\", \"but also new allies.\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This cell bwgins to clean and organize the urls.  It gets around some scrape blocking, operates within the start/stop margins, and removes unwanted characters.\r\n",
    "def collect_reviews(my_urls):\r\n",
    "    contents = \"\"\r\n",
    "    loc = 0\r\n",
    "\r\n",
    "    from urllib.request import urlopen, Request\r\n",
    " \r\n",
    "    for url in my_urls:\r\n",
    "        req = Request(url)\r\n",
    "        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:82.0) Gecko/20100101 Firefox/82.0')\r\n",
    "        webContents = urlopen(req).read().decode(\"latin-1\")\r\n",
    "        webContents = str(webContents)\r\n",
    "        \r\n",
    "        cleaned = clean_reviews(webContents, starts[loc], stops[loc])\r\n",
    "        contents += str(cleaned)\r\n",
    "        loc += 1\r\n",
    "\r\n",
    "        contents = contents.replace(\"\\\\n\",\"\")\r\n",
    "        contents = contents.replace(\"\\\\r\",\"\")\r\n",
    "        contents = contents.replace(\"\\\\t\",\"\")\r\n",
    "        \r\n",
    "        \r\n",
    "    return contents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defines a function to clean the reviews and scrubs the html.\r\n",
    "def clean_reviews(contents, start, end):\r\n",
    "    startLoc = contents.find(start)\r\n",
    "    endLoc = contents.find(end)\r\n",
    "    contents = contents[startLoc: endLoc]\r\n",
    "\r\n",
    "    inside = 0\r\n",
    "    text = ''\r\n",
    "\r\n",
    "    for char in contents:\r\n",
    "        if char == '<' or char == '{':\r\n",
    "            inside = 1\r\n",
    "        elif (inside == 1 and char == '>' or char == '}'):\r\n",
    "            inside = 0\r\n",
    "        elif inside== 1:\r\n",
    "            continue\r\n",
    "        else: \r\n",
    "            text += char\r\n",
    "\r\n",
    "    return[text]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defines a function to convert to lowercase,  normalize text with re, and return a word bag without the mes.\r\n",
    "def normalize_text(text):\r\n",
    "    text = text.lower()\r\n",
    "    import re\r\n",
    "    word_bag = re.compile(r'\\W+', re.UNICODE).split(text)\r\n",
    "    return(word_bag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defines a funtion to create a dictionary from the above word bag, zips together words and their frequwncy.\r\n",
    "def wordsToDictionary(word_bag):\r\n",
    "    word_freq = [word_bag.count(word) for word in word_bag]\r\n",
    "    return dict(list(zip(word_bag,word_freq)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sorts words by freq, most to least common\r\n",
    "def sortDictionary(counted_words):\r\n",
    "    aux = [(counted_words[key], key) for key in counted_words]\r\n",
    "    aux.sort()\r\n",
    "    aux.reverse()\r\n",
    "    return aux"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creates a list of stop words, then defines func to reomve them from word bag\r\n",
    "stopwords = ['a','n','the','my','for','with','login','facebook','functionalityfunction','src','id','parentnode','if','is','was','navigation','main','ul','menu','nav']\r\n",
    "def removeStopWords(word_bag):\r\n",
    "    return [w for w in word_bag if w not in stopwords]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "collected_text = collect_reviews(my_urls)\r\n",
    "collected_words = normalize_text(collected_text)\r\n",
    "collected_words = removeStopWords(collected_words)\r\n",
    "dict_words = sortDictionary(wordsToDictionary(collected_words))\r\n",
    "\r\n",
    "print(dict_words)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)"
  },
  "interpreter": {
   "hash": "384b16392d5df87b45fa68fd804fdd570b449e149e441c1270eb35604a8982f7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}